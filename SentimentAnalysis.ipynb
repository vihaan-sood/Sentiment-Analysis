{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the drug-review dataset found at https://www.kaggle.com/datasets/mohamedabdelwahabali/drugreview?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy pandas nltk cupy scikit-learn spacy transformers datasets\n",
    "#python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 2)\n",
      "(500, 2)\n",
      "(500, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_raw = pd.read_csv(\"archive/drug_review_train.csv\", usecols=[\"review\", \"rating\"]).to_numpy()\n",
    "test_raw = pd.read_csv(\"archive/drug_review_test.csv\", usecols=[\"review\", \"rating\"]).to_numpy()\n",
    "val_raw = pd.read_csv(\"archive/drug_review_validation.csv\", usecols=[\"review\", \"rating\"]).to_numpy()\n",
    "\n",
    "np.random.seed(50)\n",
    "\n",
    "np.random.shuffle(train_raw)\n",
    "np.random.shuffle(test_raw)\n",
    "np.random.shuffle(val_raw)\n",
    "\n",
    "train_raw = train_raw[:4000]       \n",
    "test_raw = test_raw[:500]  \n",
    "val_raw = val_raw[:500]  \n",
    "\n",
    "print(train_raw.shape)\n",
    "print(test_raw.shape)\n",
    "print(val_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"reporting in. after 5 weeks of stopping the anastrazole, my joint pain was pretty much gone and i was ecstatic! due to serious family illness, i had to postpone my follow up visit with my oncologist. fearing that i had been off the med long enough, i started taking it again. that was a week ago. the joint pain came back today with a vengeance  - aching knees, aching back, shortness of breath, barely able to walk. i cannot take this med anymore and it worries me as the alternatives (tamoxifen) are not as effective. see my doc in five days. will post again. my heart goes out to all the women posting on this forum.\"\n",
      "[0 1 1 -1 -1 1 1 0 1 -1 1 -1 1 1 1 1 -1 0 1 -1 -1 1 0 -1 0 -1 1 -1 -1 0 1\n",
      " 1 0 1 1 -1 -1 1 1 1 0 1 1 1 0 -1 1 1 1 1 1 1 1 1 1 1 0 -1 1 1 1 -1 1 1 1\n",
      " 1 -1 1 -1 1 1 1 0 -1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 -1 0 -1 -1 -1 1 -1 1 1 1\n",
      " 1 -1]\n"
     ]
    }
   ],
   "source": [
    "def seperate_ratings_and_text(data):\n",
    "    text = data[:,0]\n",
    "    ratings = data[:,1]\n",
    "    return text,ratings\n",
    "\n",
    "\n",
    "train_raw_text,train_raw_ratings = seperate_ratings_and_text(train_raw)\n",
    "test_raw_text,test_raw_ratings = seperate_ratings_and_text(test_raw)\n",
    "val_raw_text,val_raw_ratings = seperate_ratings_and_text(val_raw)\n",
    "\n",
    "\n",
    "def convert_rating_to_sentiment(rating_list):\n",
    "\n",
    "    for i in range(len(rating_list)):\n",
    "        rating = int(rating_list[i] )\n",
    "        if rating >= 7:\n",
    "            rating_list[i] = 1\n",
    "        elif rating <= 4:\n",
    "            rating_list[i] = -1\n",
    "        else:\n",
    "            rating_list[i] = 0\n",
    "\n",
    "    return rating_list\n",
    "\n",
    "convert_rating_to_sentiment(train_raw_ratings)\n",
    "convert_rating_to_sentiment(test_raw_ratings)\n",
    "convert_rating_to_sentiment(val_raw_ratings)\n",
    "\n",
    "print(train_raw_text[0])\n",
    "print(train_raw_ratings[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporting in after 5 weeks of stopping the anastrazole my joint pain was pretty much gone and i was ecstatic due to serious family illness i had to postpone my follow up visit with my oncologist fearing that i had been off the med long enough i started taking it again that was a week ago the joint pain came back today with a vengeance aching knees aching back shortness of breath barely able to walk i cannot take this med anymore and it worries me as the alternatives tamoxifen are not as effective see my doc in five days will post again my heart goes out to all the women posting on this forum\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_review(review):\n",
    "    review = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", review)\n",
    "    review = re.sub(r\"\\s+\", \" \", review) \n",
    "    review = review.lower()\n",
    "    return review\n",
    "\n",
    "def clean_text(text_list):\n",
    "    cleaned_sentences = [clean_review(sentence) for sentence in text_list]\n",
    "    return np.array(cleaned_sentences)\n",
    "\n",
    "\n",
    "\n",
    "train_cleaned = clean_text(train_raw_text)\n",
    "test_cleaned = clean_text(test_raw_text)\n",
    "val_cleaned = clean_text(val_raw_text)\n",
    "\n",
    "print(train_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 60\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m train_lem \u001b[38;5;241m=\u001b[39m \u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_cleaned\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     61\u001b[0m test_lem \u001b[38;5;241m=\u001b[39m lemmatize(test_cleaned)   \n\u001b[0;32m     62\u001b[0m val_lem \u001b[38;5;241m=\u001b[39m lemmatize(val_cleaned)    \n",
      "Cell \u001b[1;32mIn[4], line 54\u001b[0m, in \u001b[0;36mlemmatize\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     52\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[1;32m---> 54\u001b[0m     res\u001b[38;5;241m.\u001b[39mappend(\u001b[43mbetter_lemmatizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "Cell \u001b[1;32mIn[4], line 39\u001b[0m, in \u001b[0;36mbetter_lemmatizer\u001b[1;34m(single_sentence)\u001b[0m\n\u001b[0;32m     36\u001b[0m single_sentence \u001b[38;5;241m=\u001b[39m clean_review(single_sentence)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#get PoS tags for the sentence\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m text_PoS \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_sentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#remove stopwords, lemmatize\u001b[39;00m\n\u001b[0;32m     42\u001b[0m lemmatized_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     43\u001b[0m     lemmatize_word(token\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower(), token\u001b[38;5;241m.\u001b[39mpos_)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m text_PoS\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopWords\n\u001b[0;32m     46\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\spacy\\language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\spacy\\pipeline\\tok2vec.py:126\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    124\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m--> 126\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\layers\\with_array.py:42\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\layers\\with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward\u001b[1;34m(model, Xs, is_train)\u001b[0m\n\u001b[0;32m     75\u001b[0m lengths \u001b[38;5;241m=\u001b[39m NUMPY_OPS\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[0;32m     76\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[1;32m---> 77\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[0;32m     80\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\layers\\residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output \u001b[38;5;241m+\u001b[39m dX\n\u001b[1;32m---> 41\u001b[0m Y, backprop_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] \u001b[38;5;241m+\u001b[39m Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[1;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\layers\\layernorm.py:24\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(model: Model[InT, InT], X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[InT, Callable]:\n\u001b[1;32m---> 24\u001b[0m     N, mu, var \u001b[38;5;241m=\u001b[39m \u001b[43m_get_moments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     Xhat \u001b[38;5;241m=\u001b[39m (X \u001b[38;5;241m-\u001b[39m mu) \u001b[38;5;241m*\u001b[39m var \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m     26\u001b[0m     Y, backprop_rescale \u001b[38;5;241m=\u001b[39m _begin_update_scale_shift(model, Xhat)\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\thinc\\layers\\layernorm.py:75\u001b[0m, in \u001b[0;36m_get_moments\u001b[1;34m(ops, X)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_moments\u001b[39m(ops: Ops, X: Floats2d) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Floats2d, Floats2d, Floats2d]:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# TODO: Do mean methods\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     mu: Floats2d \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 75\u001b[0m     var: Floats2d \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-08\u001b[39m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Floats2d, ops\u001b[38;5;241m.\u001b[39masarray_f([X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]])), mu, var\n",
      "File \u001b[1;32mc:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:191\u001b[0m, in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where, mean)\u001b[0m\n\u001b[0;32m    186\u001b[0m         arrmean \u001b[38;5;241m=\u001b[39m arrmean \u001b[38;5;241m/\u001b[39m rcount\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# not a scalar.\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marrmean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, (nt\u001b[38;5;241m.\u001b[39mfloating, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[0;32m    194\u001b[0m     x \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mmultiply(x, x, out\u001b[38;5;241m=\u001b[39mx)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "#PoS mapping\n",
    "pos_mapping = {\n",
    "    \"NOUN\": \"n\",\n",
    "    \"PROPN\": \"n\",\n",
    "    \"VERB\": \"v\",\n",
    "    \"AUX\": \"v\",\n",
    "    \"ADJ\": \"a\",\n",
    "    \"ADV\": \"r\",\n",
    "}\n",
    "\n",
    "#cache dictionary for lemmatized words with PoS\n",
    "lemmatized_cache = {}\n",
    "\n",
    "def lemmatize_word(word, pos):\n",
    "\n",
    "    key = (word, pos)  #use word and PoS as the key\n",
    "    if key in lemmatized_cache:\n",
    "        return lemmatized_cache[key]\n",
    "\n",
    "    #compute lemmatized form and cache it\n",
    "    lemmatized_word = lemmatizer.lemmatize(word, pos_mapping.get(pos, \"n\"))\n",
    "    lemmatized_cache[key] = lemmatized_word\n",
    "    return lemmatized_word\n",
    "\n",
    "def better_lemmatizer(single_sentence):\n",
    "\n",
    "    #clean the sentence (assuming clean_review is defined)\n",
    "    single_sentence = clean_review(single_sentence)\n",
    "\n",
    "    #get PoS tags for the sentence\n",
    "    text_PoS = nlp(single_sentence)\n",
    "\n",
    "    #remove stopwords, lemmatize\n",
    "    lemmatized_list = [\n",
    "        lemmatize_word(token.text.lower(), token.pos_)\n",
    "        for token in text_PoS\n",
    "        if token.text.lower() not in stopWords\n",
    "    ]\n",
    "\n",
    "    return lemmatized_list\n",
    "\n",
    "def lemmatize(data):\n",
    "\n",
    "    res = []\n",
    "    for i in range(len(data)):\n",
    "        res.append(better_lemmatizer(data[i]))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "# Example usage\n",
    "train_lem = lemmatize(train_cleaned)  \n",
    "test_lem = lemmatize(test_cleaned)   \n",
    "val_lem = lemmatize(val_cleaned)    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['report 5 week stop anastrazole joint pain pretty much go ecstatic due serious family illness postpone follow visit oncologist fearing med long enough start take week ago joint pain come back today vengeance ache knee ache back shortness breath barely able walk take med anymore worry alternative tamoxifen effective see doc five day post heart go woman post forum', 'imagine awe use sleep even camp heat storm tent 20 maybe exhausted mountain trek 30 major issue except occasional job stress ex loud snore change 40 within 5 month mom die get divorce stab street robbery sleep sometimes almost impossible read site probably look answer love one prescribe many thing drs repeatedly tell traz best nonaddictive allow stage sleep cycle work great albeit funky dream nextday grogginess never leave many month quit absolutely withdrawal']\n",
      "4000\n",
      "['three week ago begin around 20 hot flash per day go night sweat 4 time per night 5 day ago doctor put pristiq father recently pass away several personal thing happen nt start period month either say would take care night sweat hot flash depression grief anxiety pill 5 day lose four lb go pk cigarette per day four cigarette get night sweat twice night go 21 hot flash per day 4 far good feel little tired agitation confusion cry emptiness feel like slowly get back far thankful pristiq', 'lupron shoot almost 2 year first six month hard wake tired icky feel puke lose appetite lose lot weight make insomnia bad urge pee bad especially night nausea settle appetite come back side effect easy cope top side effect implanon nt bleed pain get 3 month shooting pain come back bleeding get put back decide take break crampy start spot obgyn say nt pain like shall wait see since lupron work medicaid wo nt let lap see stage endometriosis']\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "train_lem_str = [\" \".join(row) for row in train_lem]\n",
    "test_lem_str = [\" \".join(row) for row in test_lem]\n",
    "val_lem_str = [\" \".join(row) for row in val_lem]\n",
    "\n",
    "print(train_lem_str[0:2])\n",
    "print(len(train_lem_str))\n",
    "print(test_lem_str[0:2])\n",
    "print(len(test_lem_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3),max_features=60000)\n",
    "\n",
    "train_count = vectorizer.fit_transform(train_lem_str)\n",
    "test_count = vectorizer.transform(test_lem_str)\n",
    "\n",
    "val_count = vectorizer.transform(val_lem_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import bigrams\n",
    "# from collections import Counter \n",
    "\n",
    "# def get_ngram_columns(processed_list,most_common_threshold = 1000):\n",
    "\n",
    "#     #function that finds the ngrams of a BoW tokenised list, takes in the list, value of \n",
    "#     # n (bigrams/trigrams/4-grams...). returns the most common 1000 ngrams , and ngram_columns(which is redundant)\n",
    "\n",
    "#     #dictionary of all tokens\n",
    "#     ngram_counts = Counter()\n",
    "\n",
    "#     #to find most common, find ngrams of entire corpus then update counts\n",
    "#     for review_number in range(len(processed_list)):\n",
    "#         sentence = processed_list[review_number]\n",
    "#         ngrams = list(bigrams(sentence))\n",
    "#         ngram_counts.update(ngrams)\n",
    "\n",
    "#     #get the 1000 most common ones (decided by default variable most_common_threshold)\n",
    "#     #these will be the columns in the TC matrix\n",
    "#     most_common_list = list(ngram_counts.most_common(most_common_threshold))\n",
    "\n",
    "#     return most_common_list\n",
    "\n",
    "# get_ngram_columns(test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['002 mg' '002 mg next' '0025' ... 'zyprexa work' 'zyprexa work well'\n",
      " 'zyrtec']\n",
      "(4000, 60000)\n",
      "(500, 60000)\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 81 stored elements and shape (1, 60000)>\n",
      "  Coords\tValues\n",
      "  (0, 50544)\t0.10579050674660305\n",
      "  (0, 57945)\t0.07609240220141587\n",
      "  (0, 53652)\t0.048959601711478146\n",
      "  (0, 41192)\t0.18811509159758982\n",
      "  (0, 47555)\t0.0918217986838576\n",
      "  (0, 49304)\t0.06932999663071683\n",
      "  (0, 45333)\t0.05054767971570731\n",
      "  (0, 38259)\t0.07011211732995874\n",
      "  (0, 34432)\t0.13385947364282724\n",
      "  (0, 34195)\t0.06542240289377231\n",
      "  (0, 51445)\t0.08980352521653046\n",
      "  (0, 35980)\t0.08774028596511059\n",
      "  (0, 40461)\t0.10661638198189528\n",
      "  (0, 48995)\t0.13385947364282724\n",
      "  (0, 37105)\t0.08212426198550446\n",
      "  (0, 57543)\t0.09575166696628208\n",
      "  (0, 47121)\t0.13872909241374187\n",
      "  (0, 43657)\t0.1295341581436704\n",
      "  (0, 42700)\t0.05952658602208312\n",
      "  (0, 34959)\t0.07296746476751245\n",
      "  (0, 53076)\t0.03721261085439244\n",
      "  (0, 54468)\t0.05298190791851636\n",
      "  (0, 1698)\t0.05429404495100308\n",
      "  (0, 19497)\t0.05500107933538093\n",
      "  (0, 3157)\t0.09215162618351984\n",
      "  :\t:\n",
      "  (0, 53300)\t0.06579774165491822\n",
      "  (0, 55200)\t0.09251765881522789\n",
      "  (0, 57967)\t0.08294018303963693\n",
      "  (0, 47595)\t0.1221265126950191\n",
      "  (0, 19558)\t0.07937329032096334\n",
      "  (0, 51740)\t0.11265384892349656\n",
      "  (0, 10896)\t0.14559243459063537\n",
      "  (0, 3565)\t0.13872909241374187\n",
      "  (0, 1227)\t0.11673601945535943\n",
      "  (0, 57687)\t0.13008230387751155\n",
      "  (0, 54872)\t0.09714371821037274\n",
      "  (0, 51270)\t0.13872909241374187\n",
      "  (0, 37005)\t0.1093673536464364\n",
      "  (0, 32299)\t0.13872909241374187\n",
      "  (0, 39455)\t0.1243868098713047\n",
      "  (0, 48978)\t0.13872909241374187\n",
      "  (0, 49327)\t0.13385947364282724\n",
      "  (0, 34986)\t0.13872909241374187\n",
      "  (0, 53331)\t0.13385947364282724\n",
      "  (0, 55202)\t0.1243868098713047\n",
      "  (0, 47596)\t0.13385947364282724\n",
      "  (0, 19621)\t0.14559243459063537\n",
      "  (0, 10897)\t0.14559243459063537\n",
      "  (0, 1228)\t0.13872909241374187\n",
      "  (0, 32300)\t0.14559243459063537\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(train_count.shape)\n",
    "print(test_count.shape)\n",
    "print(train_count[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Evaluation:\n",
      "Accuracy: 0.69\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       116\n",
      "           0       0.00      0.00      0.00        39\n",
      "           1       0.69      1.00      0.82       345\n",
      "\n",
      "    accuracy                           0.69       500\n",
      "   macro avg       0.23      0.33      0.27       500\n",
      "weighted avg       0.48      0.69      0.56       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(train_count, train_raw_ratings.astype(int))\n",
    "\n",
    "val_predictions = model.predict(val_count)\n",
    "\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(val_raw_ratings.astype(int), val_predictions))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_raw_ratings.astype(int), val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Now I use BERT to classify the reviews. This is different to the previous approach as it is not feature based and hence there is no need to find any PoS/Unigrams/Bigrams etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"CUDA_VISIBLE_DEVICES\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.7.0.dev20250117+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_raw_text = np.array(train_raw_text).reshape(-1, 1)\n",
    "train_raw_ratings = np.array(train_raw_ratings+1).reshape(-1, 1)\n",
    "\n",
    "test_raw_text = np.array(test_raw_text).reshape(-1, 1)\n",
    "test_raw_ratings = np.array(test_raw_ratings+1).reshape(-1, 1)\n",
    "\n",
    "val_raw_text = np.array(val_raw_text).reshape(-1, 1)\n",
    "val_raw_ratings = np.array(val_raw_ratings+1).reshape(-1, 1)\n",
    "\n",
    "train_concat_raw = np.hstack((train_raw_text, train_raw_ratings))\n",
    "test_concat_raw = np.hstack((test_raw_text, test_raw_ratings))\n",
    "val_concat_raw = np.hstack((val_raw_text, val_raw_ratings))\n",
    "\n",
    "test_df = pd.DataFrame(test_concat_raw,columns=[\"text\",\"labels\"])\n",
    "train_df = pd.DataFrame(train_concat_raw,columns=[\"text\",\"labels\"])\n",
    "val_df = pd.DataFrame(val_concat_raw,columns=[\"text\",\"labels\"])\n",
    "\n",
    "train_dict = train_df.to_dict(orient=\"list\")\n",
    "test_dict = test_df.to_dict(orient=\"list\")\n",
    "validation_dict = val_df.to_dict(orient=\"list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vihaa\\Documents\\GitHub (not onedrive)\\Sentiment-Analysis\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 4000/4000 [00:00<00:00, 6484.22 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 6195.26 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 6657.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'attention_mask']\n",
      "Batch input_ids shape: torch.Size([8, 512])\n",
      "Batch attention_mask shape: torch.Size([8, 512])\n",
      "Batch labels shape: torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 10:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.517033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.484900</td>\n",
       "      <td>0.512465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.402200</td>\n",
       "      <td>0.531707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.5256244506835938, metrics={'train_runtime': 619.0946, 'train_samples_per_second': 19.383, 'train_steps_per_second': 2.423, 'total_flos': 3157361012736000.0, 'train_loss': 0.5256244506835938, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,  TrainingArguments, Trainer\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_dict)\n",
    "test_dataset = Dataset.from_dict(test_dict)\n",
    "validation_dataset = Dataset.from_dict(validation_dict)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels = 3)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=\"max_length\")\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([ \"text\", \"token_type_ids\"])\n",
    "test_dataset = test_dataset.remove_columns([ \"text\", \"token_type_ids\"])\n",
    "validation_dataset = validation_dataset.remove_columns([ \"text\", \"token_type_ids\"])\n",
    "\n",
    "print(train_dataset.column_names)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=7e-6,\n",
    "    use_cpu=False,\n",
    "    seed = 50)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "\n",
    "\n",
    ")\n",
    "for batch in trainer.get_train_dataloader():\n",
    "    print(f\"Batch input_ids shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Batch attention_mask shape: {batch['attention_mask'].shape}\")\n",
    "    print(f\"Batch labels shape: {batch['labels'].shape}\")\n",
    "    break\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./trained_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./trained_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./trained_model\")\n",
    "trainer.save_model(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.78      0.76       129\n",
      "           1       1.00      0.03      0.07        29\n",
      "           2       0.88      0.92      0.90       342\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.87      0.58      0.57       500\n",
      "weighted avg       0.85      0.84      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "predicted_classes = np.argmax(logits, axis=-1)\n",
    "\n",
    "\n",
    "print(classification_report(labels,predicted_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
